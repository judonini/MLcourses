{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent\n",
    "\n",
    "In this exercice we will make a linear fit to simulated data using the (stochastic) gradient descent method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import optimize # for fits\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate training dataset\n",
    "\n",
    "a) First Generate N=100 observations of 1-D feature x and target values t, where:\n",
    "* x is evenly spaced between 0 and 1 (use `np.linspace` function)\n",
    "* t follows a linear function $f(x)$ plus some random gaussian noise $\\epsilon$: $$t_i = f(x_i) + \\epsilon = a \\cdot x_i + b + \\epsilon,$$ with a=2 and b=5, and $\\epsilon$ is distributed along the normal distribution `np.random.normal(0,0.1,N)`\n",
    "\n",
    "b) On a figure plot the values of the N data points $\\{x_i,t_i\\}$ and draw, on the same figure, the function $f(x)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cost function and gradients\n",
    "\n",
    "To determine the weights $a$ and $b$ of $f(x)$ we'll use the Mean Square Error cost function:\n",
    "$$E(a,b) = \\frac{1}{N} \\sum_{i=1}^N \\left(t_i - y(x_i) \\right) ^2.$$\n",
    "\n",
    "The derivatives of the cost function with respect to the parameters $a$ and $b$ are: \n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\begin{cases}\n",
    "\\frac{\\partial E(a,b)}{\\partial a} = -2 \\frac{1}{N} \\sum_{i=1}^N x_i \\left(t_i - y(x_i) \\right) \\\\\\\\\n",
    "\\frac{\\partial E(a,b)}{\\partial b} = -2 \\frac{1}{N} \\sum_{i=1}^N \\left(t_i - y(x_i) \\right) \n",
    "\\end{cases}\n",
    "\\end{eqnarray}\n",
    "\n",
    "a) Write a function `E(a,b,x,t)` that return the MSE cost function:\n",
    "```python\n",
    "def E(a,b,x,t):\n",
    "    N = len(x)\n",
    "    ...\n",
    "    return E\n",
    "```\n",
    "\n",
    "b) Write a function `update_weights` that calculate the partial derivatives of the cost function, and that updates the weights $a$ and $b$ for a given `learning_rate`:\n",
    "```python\n",
    "def update_weights(a, b, x, t, learning_rate):\n",
    "    a_deriv = 0\n",
    "    b_deriv = 0\n",
    "    N = len(x)\n",
    "    for i in range(N):\n",
    "        # Calculate partial derivatives\n",
    "        ...\n",
    "    # Update weights\n",
    "    a -= ...\n",
    "    b -= ...\n",
    "    return a, b\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Performing training\n",
    "\n",
    "a) Starting from initial values $a=1$, $b=1$, apply the gradient descent method `nsteps=1000` times, with `learning_rate=0.05`. Calculate the cost function at each step. What are the values of $a$ and $b$ at the last step ?\n",
    "\n",
    "b) Make a figure of the cost function as a function of number of steps.\n",
    "\n",
    "c) Show on a 2-D figure how the value of the parameters $a$ and $b$ change at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Stochastic gradient descent\n",
    "\n",
    "a) Perform gradient descent on batch of 10 events instead of total number of events $N$. This is called ${\\it stochastic}$ gradient descent.\n",
    "\n",
    "b) Redo the same figures as in question 3.\n",
    "\n",
    "c) Play with batch size and learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "nsteps=1000\n",
    "a_grad=1 # initial value for a\n",
    "b_grad=1 # initial value for b\n",
    "learning_rate = 0.05\n",
    "batch_size=1 # batch size\n",
    "\n",
    "# First pair x,t values\n",
    "x1 = x[:,np.newaxis] # Transform vector into column of dim (100,1)\n",
    "t1 = t[:,np.newaxis] # Transform vector into column of dim (100,1)\n",
    "points = np.concatenate((x1,t1),axis=1) # Concatenate two vectors in table of dim (100,2)\n",
    "\n",
    "# Training\n",
    "va=[]\n",
    "vb=[]\n",
    "cost=[]\n",
    "for i in range(nsteps):\n",
    "    np.random.shuffle(points)   # Shuffle values\n",
    "    XX = points[:batch_size,0]  # x feature\n",
    "    YY = points[:batch_size,1]  # target value\n",
    "    # FILL HERE\n",
    "\n",
    "# CONTINUE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Fit data with a polynomial function\n",
    "\n",
    "Check that the fitted parameters are compatible with the true parameters $a$ and $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
