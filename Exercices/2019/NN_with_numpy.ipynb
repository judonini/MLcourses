{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Building a NN using numpy\n",
    "\n",
    "A fully-connected ReLU network with one hidden layer and no biases, trained to\n",
    "predict y from x using Euclidean error.\n",
    "\n",
    "The model that we want to build has the following structure:\n",
    "$$\\hat{y}(x) = \\text{relu}(x.w_1).w_2,$$\n",
    "where $x$ and $y$ are the input and output features (of dimension 1000 and 10, respectively). Here the relu activation function is used and $w_1$ and $w_2$ are weight matrices.\n",
    "\n",
    "This implementation uses numpy to manually compute the forward pass, loss, and\n",
    "backward pass. A numpy array is a generic n-dimensional array; it does not know anything about\n",
    "deep learning or gradients or computational graphs, and is just a way to perform\n",
    "generic numeric computations.\n",
    "\n",
    "This example is adapted from: https://pytorch.org/tutorials/beginner/pytorch_with_examples.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward and backward pass\n",
    "\n",
    "Forward pass: $x \\rightarrow h= x.W_1 \\rightarrow \\hat{y} = \\text{relu}(h).W_2.$\n",
    "\n",
    "Cost and loss functions:\n",
    "* Cost: $E(W) = \\sum_{i=1}^N (\\hat{y} - y)^2$\n",
    "* $\\text{loss}: \\ell(\\hat{y},W) = (\\hat{y} - y)^2.$\n",
    "\n",
    "Backward pass: derivatives of loss function\n",
    "\n",
    "$$\\frac{\\partial \\ell}{\\partial W_2} = \\frac{\\partial \\ell}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial W_2} = 2(\\hat{y} - y).\\text{relu}(h)$$\n",
    "\n",
    "$$\\frac{\\partial \\ell}{\\partial W_1} = \\frac{\\partial \\ell}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial h} \\frac{\\partial h}{\\partial W_1} = 2(\\hat{y} - y).W_2. \\frac{\\partial \\text{relu}(h)}{\\partial h} .x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "N = 64      # N: input batch size\n",
    "D_in = 1000 # D_in: input dimension\n",
    "H = 100     # H: hidden layer dimension;\n",
    "D_out = 10  # D_out: output dimension\n",
    "\n",
    "# Create random input and output (target) training data\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "# Randomly initialize weights (no bias terms)\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. First let's look at the dimension of data and weights. \n",
    "For this use e.g. print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Forward pass: compute predicted y\n",
    "\n",
    "We want to compute: $\\hat{y} = \\text{relu}(x.w_1).w_2$, where the relu activation function is used.\n",
    "\n",
    "For this calculate (be careful of the matrix dimensions):\n",
    "* h = dot product of x and w1 (use .dot() function)\n",
    "* h_relu: $\\text{relu}(h)$ relu activation function (using np.maximum() function)\n",
    "* compute $\\hat{y}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = x.dot(w1)       # .dot() = matrix multiplication               \n",
    "h_relu = np.maximum(# FILL HERE #)    \n",
    "y_pred = h_relu.dot(# FILL HERE #)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Calculate and print cost function\n",
    "* Cost: $E(w_1,w_2) = \\sum_{i=1}^N (\\hat{y_i} - y_i)^2$\n",
    "* $\\text{loss}: \\ell(\\hat{y},W) = (\\hat{y} - y)^2.$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Backward pass\n",
    "Compute gradients of $w_1$ and $w_2$ with respect to loss.\n",
    "\n",
    "Beware of matrices dimensions !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have: dim(x) = (64, 1000); dim($w_1$) = (1000, 100); dim($w_2$) = (100, 10); dim(y) = (64, 10). Therefore:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\ell}{\\partial W_2} = [\\text{relu}(h)]^T \\cdot 2(\\hat{y} - y) & \\ \\rightarrow \\ \\text{dim = } (100,64) \\times (64,10) = (100,10) \\\\\n",
    "\\frac{\\partial \\ell}{\\partial W_1} = x^T \\cdot \\left( \\frac{\\partial \\text{relu}(h)}{\\partial h} \\cdot 2 (\\hat{y} - y) \\right) \\cdot W_2^T & \\ \\rightarrow \\ \\text{dim = } (1000,64) \\times (64,10) \\times (10,100) = (1000,100) \\\\\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_y_pred =   # FILL HERE #\n",
    "grad_w2 =       # FILL HERE #\n",
    "grad_h_relu =   # FILL HERE #\n",
    "grad_h = grad_h_relu.copy()\n",
    "grad_h[# CONDITION #] = # FILL VALUE #   # Here we use this trick to change grad_h values when a condition is realized\n",
    "grad_w1 =       # FILL HERE #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Update weights $w_1$ and $w_2$\n",
    "Note: because of matrices dimensionality the gradients are already summed over all N data events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Repeat procedure\n",
    "\n",
    "The above procedure will be repeated 500 times. In addition we'll test the model on a validation set that is created in the same way as the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation set\n",
    "x_val = np.random.randn(N, D_in)\n",
    "y_val = np.random.randn(N, D_out)\n",
    "\n",
    "# Counters\n",
    "cost=[]\n",
    "counter=[]\n",
    "cost_val=[]\n",
    "\n",
    "niteration = 500\n",
    "\n",
    "for t in range(niteration):\n",
    "    # CONTINUE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Plot model performance\n",
    "Using matplotlib plot the evolution of cost as a function of the number of iterations. Do this for both training and validation datasets (on same figure).\n",
    "\n",
    "Conclude on the generalization of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion ?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
