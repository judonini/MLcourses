{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82b9cf93",
   "metadata": {},
   "source": [
    "# Classification using pytorch\n",
    "\n",
    "In this exercise we will use pytorch to perform a 2-classes classification exercice. For this we will first implement the logistic regression method and observe that it performs rather poorly. Then we will implement a simple neural network and show that the classification can be greatly improved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccfc4cd",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5efca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, QuantileTransformer\n",
    "\n",
    "print('Run on cpu')\n",
    "dtype = torch.FloatTensor\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# fix random generators seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(seed=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8edde03",
   "metadata": {},
   "source": [
    "# 1. Generation of the samples\n",
    "\n",
    "Just run the few cells below to generate the two classes: a 'background' sample and a 'signal' sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d75c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "Nsample = 10000 # Number of events in each sample\n",
    "features=[\"x0\",\"x1\",\"x2\",\"x3\",\"x4\",\"x5\"] # feature names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9aa730f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation of background sample\n",
    "# The first 5 features are samples from a multi-dimensional gaussian function. \n",
    "# The 6-th feature is constructed from the 5 others.\n",
    "\n",
    "covMatrix = np.array([[ 0.81652219,  0.12798401,  0.25894314, -0.65065863,  1.24887103],\n",
    "       [ 0.12798401,  0.90369306, -0.0297507 , -0.27944877,  2.31268408],\n",
    "       [ 0.25894314, -0.0297507 ,  1.27407394,  0.5200441 , -0.0738098 ],\n",
    "       [-0.65065863, -0.27944877,  0.5200441 , 19.43674212, -5.04547895],\n",
    "       [ 1.24887103,  2.31268408, -0.0738098 , -5.04547895, 23.93837133]])\n",
    "\n",
    "Nvar = 5\n",
    "mean = np.random.randn(Nvar)\n",
    "cov = covMatrix\n",
    "x = np.random.multivariate_normal(mean, cov, Nsample)\n",
    "\n",
    "# 6-th feature\n",
    "m = np.sqrt(np.sum(x**2,axis=1))\n",
    "m = m.reshape(Nsample,1)\n",
    "\n",
    "# Put sample in a panda dataframe\n",
    "data = np.concatenate((x,m),axis=1)\n",
    "background = pd.DataFrame(data,columns=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64556a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we generate the signal sample\n",
    "# We use a custom function from scikit-learn that generates 'blobs' of data\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "centers = [[0,2,-2,0,0,6]] # center of the blobs\n",
    "X, y = make_blobs(n_samples=Nsample, cluster_std=2.5, centers=centers, n_features=6, random_state=40)\n",
    "\n",
    "signal = pd.DataFrame(X,columns=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac52198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we add a column of label to each sample: 0=background, 1=signal\n",
    "\n",
    "z=pd.DataFrame(np.zeros(len(background)))\n",
    "background[\"label\"]=pd.DataFrame(z)\n",
    "\n",
    "o=pd.DataFrame(np.ones(len(signal)))\n",
    "signal[\"label\"]=pd.DataFrame(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64227d27",
   "metadata": {},
   "source": [
    "## 2. Data exploration\n",
    "\n",
    "a) Make histograms of each features for both classes\n",
    "\n",
    "b) Look at how the features are correlated, for both classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e33e8eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a98eda03",
   "metadata": {},
   "source": [
    "# 3. Prepare the data for pytorch\n",
    "\n",
    "First we separate each sample in three data sets: train, validation and test, with fractions 40%, 20%, 40% respectively.\n",
    "\n",
    "Then we feed these datasets to the DataLoader utility that is quite helpful to split the data in batches in view of the training.\n",
    "\n",
    "Just run the two cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a3d3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â Separate train, validation and test data (40,20,40)\n",
    "\n",
    "x_background_train,x_background_test = train_test_split(background.values,test_size=0.4) \n",
    "x_background_train_train,x_background_train_valid = train_test_split(x_background_train,test_size=0.33) \n",
    "\n",
    "x_signal_train,x_signal_test = train_test_split(signal.values,test_size=0.4) \n",
    "x_signal_train_train,x_signal_train_valid = train_test_split(x_signal_train,test_size=0.33) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6a5875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=np.concatenate((x_background_train_train,x_signal_train_train)),\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(dataset=np.concatenate((x_background_train_valid,x_signal_train_valid)),\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=np.concatenate((x_background_test,x_signal_test)),\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86eeb9d",
   "metadata": {},
   "source": [
    "# 4. Build models\n",
    "\n",
    "1) Construct a first model class that implements the logistic regression method.\n",
    "- input layer (6 nodes)\n",
    "- output layer (1 node)\n",
    "- Sigmoid activation function\n",
    "\n",
    "2) Construct another model class that implements a simple MLP with the following structure:\n",
    "- input layer (6 nodes)\n",
    "- hidden layer with 200 nodes\n",
    "- ReLU activation function\n",
    "- output layer (1 node)\n",
    "- Sigmoid activation function\n",
    "\n",
    "As an example you can look this page: https://pytorch.org/tutorials/beginner/introyt/modelsyt_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d77d809",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim, hidden_dim, output_dim = 6, 200, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6802936f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression\n",
    "class LogRegression(nn.Module):\n",
    "    \"\"\"FILL HERE\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7222bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP with one hidden layer\n",
    "class MLP(nn.Module):\n",
    "    \"FILL HERE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ff440c",
   "metadata": {},
   "source": [
    "# 5. Train model\n",
    "\n",
    "After choosing some setting for the training run the training on your model (either the Log regr ot the MLP)\n",
    "\n",
    "a) Complete the cell below.\n",
    "\n",
    "b) Modify the code to store values of train and validation loss values at each epoch\n",
    "\n",
    "c) Run the training and make a plot of the loss values for the training and validation steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42d25a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings\n",
    "\n",
    "# Choose and initialise your model\n",
    "model = \"\"\" FILL HERE \"\"\"\n",
    "\n",
    "# Choose number of epochs\n",
    "num_epochs = 50\n",
    "\n",
    "# Setup the Binary Cross Entropy loss function\n",
    "criterion = nn.BCELoss() \n",
    "\n",
    "# Optimisation model, here we choose Adam which is stochastic gradient descent with improvements\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001, weight_decay = 1e-5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b1efc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    model.train() # prep model for training\n",
    "    for data in train_loader:\n",
    "        data = data.type(dtype)\n",
    "        # ===================forward=====================\n",
    "        output = model(\"\"\" FILL HERE \"\"\")[:,0]\n",
    "        target = data[\"\"\" FILL HERE \"\"\"]\n",
    "        loss = criterion(output, target)\n",
    "        # ===================backward====================\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    model.eval() # prep model for evaluation\n",
    "    for data in valid_loader:\n",
    "        \"\"\" FILL HERE\n",
    "        The main difference with the training\n",
    "        is that there is no optimization of the weights of the mode \"\"\"\n",
    "\n",
    "\"\"\"FILL HERE\n",
    "Modify the code above to store values of train and validation \n",
    "loss values at each epoch. use them to plot the model performance \n",
    "once the training is done.\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabd28fa",
   "metadata": {},
   "source": [
    "# 6. Test phase and results\n",
    "\n",
    "For both models (logistic regression then MLP)\n",
    "\n",
    "a) Apply the trained model to test samples\n",
    "\n",
    "b) Show the histograms of the model output for both background and signal samples. \n",
    "\n",
    "c) Display the confusion matrix (threshold = 0.5)\n",
    "\n",
    "d) Show the ROC curve\n",
    "\n",
    "e) Conclusions: Is the separation satisfactory for the case of the logistic regression ? What about the performance for the MLP ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5fbe20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
