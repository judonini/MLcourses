{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# California Housing Data\n",
    "\n",
    "The objective of this execice is to build a linear model to predict the homes price given a set of feature. \n",
    "\n",
    "This dataset was derived from the 1990 U.S. census, using one row per census block group. A block group is the smallest geographical unit for which the U.S. Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people).\n",
    "\n",
    "Features\n",
    "* MedInc median income in block group\n",
    "* HouseAge median house age in block group\n",
    "* AveRooms average number of rooms per household\n",
    "* AveBedrms average number of bedrooms per household\n",
    "* Population block group population\n",
    "* AveOccup average number of household members\n",
    "* Latitude block group latitude\n",
    "* Longitude block group longitude\n",
    "\n",
    "The target variable is the median house value for California districts, expressed in hundreds of thousands of dollars ($100,000).\n",
    "\n",
    "A household is a group of people residing within a home. Since the average number of rooms and bedrooms in this dataset are provided per household, these columns may take surprisingly large values for block groups with few households and many empty houses, such as vacation resorts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries\n",
    "\n",
    "Libraries needed for this exercice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pandas import read_csv\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the dataset\n",
    "\n",
    "We will import the housing data from the scikit-learn library. The data comes in the form of a dictionary-like object.\n",
    "\n",
    "a) How many lines and column does this dataset have ? Show the first 5 examples.\n",
    "\n",
    "b) Check there are no missing values. For this use `isnull().sum()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get dataset\n",
    "housing = fetch_california_housing(as_frame=True)\n",
    "\n",
    "# a) Show some examples\n",
    "\n",
    "\n",
    "# b) Check if there are any missing values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data exploration\n",
    "\n",
    "We first assign features values to numpy array $X$ and target values to numpy array $y$\n",
    "\n",
    "a) check the dimension of $X$ and $y$\n",
    "\n",
    "b) Make histograms for each features and for the target\n",
    "\n",
    "c) Show scatter plots of each feature vs the target (optional: calculate correlation coefficient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data\n",
    "X = housing.data.to_numpy()   # Features\n",
    "y = housing.target.to_numpy() # Target\n",
    "\n",
    "# a) X and y dimensions\n",
    "\n",
    "\n",
    "# b) Show histograms\n",
    "\n",
    "\n",
    "# c) Scatter plots\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Split data in train and test samples\n",
    "\n",
    "We now split the total dataset in a train and a test sample using scikit-learn.\n",
    "\n",
    "Look at the size of each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Linear regression\n",
    "\n",
    "Now let's construct a predictive model using linear regression:\n",
    "\n",
    "$$y_{pred} = w_0 + \\sum_{i=1}^{N=13} w_i X_i$$\n",
    "\n",
    "For this we use the scikit-learn model described here: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "\n",
    "a) Fit the linear regression model using the training dataset and print the parameters (weights and bias term) of the fit.\n",
    "\n",
    "b) Get the predicted model output, `y_train_pred`, using the training dataset. Make a scatter plot of the true target value, `y_train`, vs the predicted value, `y_train_pred`. Then, plot the difference `(y_train - y_train_pred)` in a histogram.\n",
    "\n",
    "c) Calculate the root mean square error (RMS) between `y_train` and `y_train_pred`. For this you can use the scikit-learn function `mean_squared_error()`.\n",
    "\n",
    "d) Finally we apply the model to the test dataset: repeat steps b) and c) with the test sample. Do you think that the model is acceptable ? Is there an overfitting problem ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit of the model\n",
    "model1 = LinearRegression()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Ridge penalty (a.k.a L2 norm)\n",
    "\n",
    "Let's see if a penalized linear algorithm can improve the modelling and prediction of the data. For this we use Ridge regression (also called L2 norm) which adds a penalty term to the fit model:\n",
    "\n",
    "$$y_{pred} = w_0 + \\sum_{i=1}^{N=13} w_i X_i + \\lambda \\sum_{i=0}^{N=13} w_i^2$$\n",
    "\n",
    "See the scikit-learn implementation https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html\n",
    "\n",
    "a) Train the model using the training dataset and a $\\lambda$ regularization parameter =1\n",
    "\n",
    "b) Apply the algorithm to the test data and check the quality of the model. Do you see any improvement in the data modelling and prediction ? Try other values of $\\lambda$.\n",
    "\n",
    "c) Optional, try Lasso penalty: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html. Does it help ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model2 = Ridge(alpha=1) # Alpha sets the lambda (yes...) hyperparameter \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Estimating model performance: Cross-validation\n",
    "\n",
    "Instead of splitting the dataset in one training and one test samples we can use cross-validation to better determine the performance of a fit model. For this we apply the following [procedure](https://machinelearningmastery.com/k-fold-cross-validation/):\n",
    "- Shuffle the dataset randomly.\n",
    "- Split the dataset into k groups\n",
    "- For each unique group:\n",
    "  - Take the group as test data set\n",
    "  - Take the k-1 remaining groups as a training data set\n",
    "  - Fit a model on the training set and evaluate it on the test set\n",
    "  - Retain the evaluation score and discard the model\n",
    "- Summarize the skill of the model using the sample of model evaluation scores\n",
    "\n",
    "a) Look at the example below, what are the different parameters ? To what corresponds the output ?\n",
    "\n",
    "b) Apply the cross-validation to the other models. Can you say if one is more performant than the other ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=False)\n",
    "scores = cross_val_score(model1, X, y, scoring='neg_mean_squared_error', cv=cv)\n",
    "scores = np.absolute(scores)\n",
    "print('Mean RMS: %.2f +- %.2f' % (np.mean(np.sqrt(scores)),np.std(np.sqrt(scores))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Optional: Improve the model ?\n",
    "\n",
    "A much better result can be achieved by pre-processing the data and adding some non-linerity to the model with using a neural network \n",
    "\n",
    "a) First, standardize all features by removing the mean and scaling to unit variance using [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html).\n",
    "\n",
    "b) Try to implement a neural network with [Multi-layer Perceptron regressor](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html#sklearn.neural_network.MLPRegressor). For example, two hidden layers with 100 and 50 neurons each and relu activation function does a decent job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
