{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder for credit card detection (Pytorch)\n",
    "\n",
    "The objective of this exercice is to build a model able to detect fraudulous credit card transactions among normal transactions. For this we train a special type of neural network called autoencoder. This network has as many input nodes as output nodes, and several hidden layers with, usually lower dimensions. \n",
    "\n",
    "The dataset we're going to use can be downloaded from [Kaggle](https://www.kaggle.com/mlg-ulb/creditcardfraud) (big file: 144 MB). It contains data about credit card transactions that occurred during a period of two days, with 492 frauds out of 284,807 transactions.\n",
    "\n",
    "All 30 features in the dataset are numerical. The data has been transformed using PCA transformation(s) due to privacy reasons. The two features that haven't been changed are Time and Amount. Time contains the seconds elapsed between each transaction and the first transaction in the dataset.\n",
    "\n",
    "The dataset also contains the class of event: 0 = normal transaction; 1 = fraudulous transaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import sys\n",
    "print(sys.version)\n",
    "print(torch.version)\n",
    "print('cuda:',torch.version.cuda)\n",
    "\n",
    "# Choose cpu/gpu\n",
    "use_gpu=1\n",
    "if (use_gpu):\n",
    "    print('\\nEnable gpu')\n",
    "    dtype = torch.cuda.FloatTensor\n",
    "    device = torch.device(\"cuda\") # Uncomment this to run on GPU\n",
    "    \n",
    "else:\n",
    "    print('\\nRun on cpu')\n",
    "    dtype = torch.FloatTensor\n",
    "    device = torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Explore Data\n",
    "\n",
    "a) Download the dataset and load it in a panda dataframe. Look at the first 10 examples.\n",
    "\n",
    "b) Separate the data in two classes `normal` and `fraud`, then remove the class label from these datasets in order to conserve only the features.\n",
    "\n",
    "c) Plot the first 5 features of both normal and fraud data (plotting all features is time consuming).\n",
    "\n",
    "d) Split the `normal` dataset into a training and a test sample (each of same size).\n",
    "\n",
    "After the last step you should have 3 datasets:\n",
    "* normal data used for training\n",
    "* normal data used for testing\n",
    "* fraud data used for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Rescale data\n",
    "\n",
    "Since features have different range we apply a transformation to each feature. For this we  use the MinMaxScaler that scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one:\n",
    "\n",
    "See: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html\n",
    "\n",
    "a) Fit and transofrm the training dataset using the scaler with the `fit_transform` method.\n",
    "\n",
    "b) Apply the transformation on the tests samples using the `transform` method.\n",
    "\n",
    "c) Plot the first 5 features of the normal and fraud test data and see how they changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Partition training data\n",
    "\n",
    "After all of this, it's important to partition the data. In order for your model to generalize well, you split the training data into two parts: a training and a validation set. You will train your model on 80% of the data and validate it on 20% of the remaining training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. AutoEncoder model\n",
    "\n",
    "Now we create the AutoEncoder model. \n",
    "\n",
    "Complete the network structure below using linear functions `nn.Linear(dim1,dim2)` (where `dim1` is the input dim of the layer and `dim2` the dimension of the layer output) and sigmoid activation functions `nn.Sigmoid()`:\n",
    "\n",
    "a) in the encoding part create layers of dimension 30 (input) - 30 (hidden layer 1) - 25 (hidden layer 2) - 20 (latence space), each with a sigmoid activation function\n",
    "\n",
    "b) in the decoding part create layers of dimension 25 (hidden layer 2) - 30 (hidden layer 1) - 30 (output) , where only the 1st layer has a sigmoid activation function\n",
    "\n",
    "c) look at the forward function, what does it return ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "batch_size = 2048\n",
    "hidden_layer1 = 30\n",
    "hidden_layer2 = 25\n",
    "encoding_dim = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoEncoder structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = x_train_train.shape[1]\n",
    "\n",
    "class autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            # FILL HERE\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            # FILL HERE\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Settings\n",
    "if (use_gpu):\n",
    "    model = autoencoder().cuda() # enable GPU\n",
    "else:\n",
    "    model = autoencoder()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Set the data loading utilities\n",
    "\n",
    "We now call the [DataLoader](https://pytorch.org/docs/stable/data.html) constructors for the following datasets:\n",
    "* normal data used for training\n",
    "* normal data used for validation\n",
    "* normal data used for testing\n",
    "* fraud data used for testing\n",
    "\n",
    "We shuffle the loading process of the train and validation datasets to make the learning process independent of data orderness, but the order of test datasets remains the same to examine whether we can handle unspecified bias order of inputs.\n",
    "\n",
    "See how this is done below (you need to replace your own dataset names where appropriate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For training on normal samples\n",
    "train_loader = torch.utils.data.DataLoader(dataset=#FILL HERE#,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(dataset=#FILL HERE#,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "\n",
    "# For testing on fraud examples (shuffle=False)\n",
    "test_fraud_loader = torch.utils.data.DataLoader(dataset=#FILL HERE#,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)\n",
    "\n",
    "# For testing on unseen normal sample (shuffle=False)\n",
    "test_normal_loader = torch.utils.data.DataLoader(dataset=#FILL HERE#,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training on normal samples\n",
    "\n",
    "Run the training of the network on the training sample. For this complete the code below by answering the following questions:\n",
    "\n",
    "a) Choose the mean square error loss function. See https://pytorch.org/docs/master/nn.html#loss-functions\n",
    "\n",
    "b) Select the Adam optimizer (= minimization) method with a learning rate of 0.001. See https://pytorch.org/docs/stable/optim.html.\n",
    "\n",
    "c) Fill the validation step knowing that it is the same structure as the training step but without the minimization part (not needed for validation).\n",
    "\n",
    "d) Record for each epoch the loss value calculated for the training and validation steps. Make a figure of the training and validation losses as a function of the number of epochs. Do the two curve agree ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = # FILL HERE #\n",
    "\n",
    "optimizer = # FILL HERE#\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    model.train() # prepare model for training\n",
    "    for data in train_loader:\n",
    "        data = data.type(dtype)\n",
    "        output = model(data)\n",
    "        loss = criterion(output, data)       \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    model.eval() # prepare model for evaluation\n",
    "    for data in valid_loader:\n",
    "        \n",
    "        # FILL HERE#\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Calculate autoencoder distances\n",
    "\n",
    "Now we calculate the euclidean distance between the autoencoder input and output.\n",
    "\n",
    "$$ \\text{distance} = \\sqrt{ ||x_{\\text{input}} - x_{\\text{output}}||^2} = \\sqrt{ \\sum_i (x^i_{\\text{input}} - x^i_{\\text{output}})^2}$$\n",
    "\n",
    "a) See below how this is done for the normal test data, and do the same for the fraud test data.\n",
    "\n",
    "b) Plot the histograms of the calculated distances of the normal and fraud test data. For better viewing choose a logarithmic scale for the y axis. Comment on the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.eval() # Sets the module in evaluation mode.\n",
    "model.cpu()  # Moves all model parameters and buffers to the CPU to avoid out of memory\n",
    "\n",
    "# Normal test dataset\n",
    "#--------------------\n",
    "test_normal_distance = []\n",
    "for data in test_normal_loader:\n",
    "    data = data.type(dtype).cpu().detach()\n",
    "    output = model(data)\n",
    "    test_normal_distance += torch.sqrt((torch.sum((data-output)**2,axis=1)))\n",
    "\n",
    "# convert list to tensor\n",
    "test_normal_distance = torch.FloatTensor(test_normal_distance)\n",
    "\n",
    "# convert tensor to numpy array\n",
    "test_normal_distance = test_normal_distance.numpy()\n",
    "\n",
    "# Fraud test dataset\n",
    "#-------------------\n",
    "\n",
    "# FILL HERE #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Confusion matrix\n",
    "\n",
    "Build a confusion matrix with a threshold on the distance such that 50% of fraud transactions are detected. What is the true positive rate in this case ? Is this threshold interesting ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ROC Curve\n",
    "\n",
    "Draw the ROC curve for the test sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Optimise the performance of the NN (optional)\n",
    "\n",
    "Try the following:\n",
    "* Change hyperparameters values\n",
    "* Modify activation functions\n",
    "* Add one or more layers\n",
    "* Try [dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#torch.nn.Dropout)\n",
    "* ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
